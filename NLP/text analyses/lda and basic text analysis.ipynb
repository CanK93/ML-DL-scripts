{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "from nltk import NaiveBayesClassifier\n",
    "import nltk.classify\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ans = pd.read_csv(\"feedbacks_tags.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_comment = pd.Series(df_ans['name'].tolist()).astype(str)\n",
    "dist_train = mob_comment.apply(len)\n",
    "dist_train = dist_train[dist_train>10]\n",
    " \n",
    "\n",
    "pal = sns.color_palette()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(dist_train, bins=35, range=[0, 35], normed=True)\n",
    "plt.title('Normalised histogram of character count in questions', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of characters', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_train = mob_comment.apply(lambda x: len(x.split(' ')))\n",
    "dist_train = dist_train[dist_train>3]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(dist_train, bins=16, range=[1, 6], color=pal[2], normed=True, label='train')\n",
    "plt.title('Normalised histogram of word count', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(mob_comment.astype(str)))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords.append('все')\n",
    "stopWords.append('очень')\n",
    "stopWords.append('это')\n",
    "stopWords.append('все')\n",
    "stopWords.append('всем')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mob_comment = df_ans[['count', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mob_comment.mobile_app_comment = mob_comment.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_comment.name.str.split(expand=True).stack().value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mob_comment.mobile_app_comment = mob_comment.mobile_app_comment.str.replace(',', ' ')\n",
    "mob_comment.mobile_app_comment = mob_comment.mobile_app_comment.str.replace('.', ' ')\n",
    "mob_comment.mobile_app_comment = mob_comment.mobile_app_comment.str.replace('!', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_comment['name'] = mob_comment['name'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopWords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_comment.name.str.split(expand=True).stack().value_counts()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # сначала токенизируем по предложению, потом по словам, чтобы сохранить \"особые\" слова\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # удалим все самое плохое, знаки препинания\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9А-Яа-я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9А-Яа-я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in mob_comment.mobile_app_comment:\n",
    "    allwords_stemmed = tokenize_and_stem(i) \n",
    "    totalvocab_stemmed.extend(allwords_stemmed) \n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('Всего ' + str(vocab_frame.shape[0]) + ' слов в словаре')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer( max_features=20000, min_df=1,  max_df=0.99,                                 \n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))\n",
    "\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mob_comment.name)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "print(mob_comment['count'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mob_comment = mob_comment.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут не работает\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "range_n_clusters = [ 2 , 5, 7 , 10, 15]            # clusters range you want to select\n",
    "best_clusters = 0                       \n",
    "previous_silh_avg = 0.0\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(tfidf_matrix)\n",
    "    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "    print (silhouette_avg)\n",
    "    if silhouette_avg > previous_silh_avg:\n",
    "        previous_silh_avg = silhouette_avg\n",
    "        best_clusters = n_clusters\n",
    "print (best_clusters)\n",
    "# Final Kmeans for best_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = {'np': list(mob_comment['count']), 'comment': list(mob_comment.name), 'cluster': clusters}\n",
    "\n",
    "#frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'cluster'])\n",
    "frame = pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame['np'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "\n",
    "grouped.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отсортирует комментарии по близости к кластеру\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Кластер %d:\" % i, end='')\n",
    "    cust_word = list()\n",
    "    for ind in order_centroids[i, :8]: #replace 6 with n words per cluster\n",
    "        cust_word.append(vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0])\n",
    "    \n",
    "    print(set(cust_word))    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация дендограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 150)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=list(frame.comment));\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LDA вероятностная модель, которая предполагает что документы это смесь топиков, и каждое слово принадлежит топику документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # сначала токенизируем по предложению, потом по словам, чтобы сохранить \"особые\" слова\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # удалим все самое плохое, знаки препинания\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9А-Яа-я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9А-Яа-я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities \n",
    "\n",
    "%time preprocess = [strip_proppers(doc) for doc in mob_comment.mobile_app_comment]\n",
    "\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
    "\n",
    "#remove stop words\n",
    "%time texts = tokenized_text #[[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# to create the bigrams\n",
    "bigram_model = Phrases(texts)\n",
    "import codecs\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open('bigram_sentences_all.txt', 'w', encoding='utf_8') as f:        \n",
    "        for unigram_sentence in texts:            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])            \n",
    "            f.write(bigram_sentence + '\\n')\n",
    "            \n",
    "from gensim.models.word2vec import LineSentence\n",
    "bigram_sentences = LineSentence('bigram_sentences_all.txt')\n",
    "\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "triram_sentences = []\n",
    "for bigram_sentence in bigram_sentences:\n",
    "    triram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "    triram_sentences.append(triram_sentence)\n",
    "#dictionary = Dictionary(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triram_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [tokenize_only(text) for text in triram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_text)\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.95)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_text]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics_now = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda = models.LdaModel(corpus, num_topics = num_topics_now, id2word=dictionary,\\\n",
    "                            random_state = 1024, \\\n",
    "                            update_every=5, \\\n",
    "                            chunksize=10000, \\\n",
    "                            passes=100)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "\n",
    "#topic_words = topics_matrix[:,:,1]\n",
    "topic_num = 0\n",
    "for i in range(0,num_topics):\n",
    "    print('Топик №', i, end = ': ')\n",
    "    for k in range(0,10):\n",
    "        print(topics_matrix[i][1][k][0], end=', ')\n",
    "    print()\n",
    "    topic_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.save('lda_simple_aproach.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_ans[['np_answer', 'mobile_app_comment']][df_ans.mobile_app_comment.isnull() == False]\n",
    "list(temp.iloc[[14]].mobile_app_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dictionary.doc2bow(texts[14])  \n",
    "doc_lda = lda.get_document_topics(doc)\n",
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = lda.top_topics(corpus, topn=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb биграммы\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigtam_sen = pd.concat([trigram_sen, df], axis=1)\n",
    "trigtam_sen = trigtam_sen[trigram_sen.sentences.str.strip().str.len() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Доказа́тельство с нулевы́м разглаше́нием (информа́ции) в криптографии (англ. Zero-knowledge proof) —  \\\n",
    "интерактивный криптографический протокол, позволяющий одной из взаимодействующих сторон («The verifier» — проверяющей) убедиться \\\n",
    "в достоверности какого-либо утверждения (обычно математического), не имея при этом никакой другой информации от второй стороны \\\n",
    "(«The prover» — доказывающей). Причём последнее условие является необходимым, так как обычно доказать, \\\n",
    "что сторона обладает определёнными сведениями в большинстве случаев тривиально, если она имеет право просто \\\n",
    "раскрыть информацию. Вся сложность состоит в том, чтобы доказать, что у одной из сторон есть информация, \\\n",
    "не раскрывая её содержание. Протокол должен учитывать, что доказывающий сможет убедить проверяющего только \\\n",
    "в случае, если утверждение действительно доказано. В противном случае сделать это будет невозможно,\\\n",
    "или крайне маловероятно из-за вычислительной сложности.\\\n",
    "Под интерактивностью протокола подразумевается непосредственный обмен информацией сторонами[1][2]. \\\n",
    "Таким образом, рассматриваемый протокол требует наличия интерактивных исходных данных (interactive input) \\\n",
    "от проверяющего, как правило, в виде задачи или проблемы. Цель легального доказывающего (имеющего доказательство) \\\n",
    "в этом протоколе — убедить проверяющего в том, что у него есть решение, не выдав при этом даже части «секретного» \\\n",
    "доказательства («нулевое разглашение»). Цель проверяющего же — это удостовериться в том, что доказывающая сторона \\\n",
    "«не лжёт»[2][3].\\\n",
    "Также были разработаны протоколы доказательства с нулевым разглашением[4][5], для которых не требовалось \\\n",
    "наличия интерактивных исходных данных, при этом доказательство которых, как правило, опирается на предположение об \\\n",
    "идеальной криптографической хеш-функции, то есть предполагается, что выход однонаправленной хеш-функции невозможно \\\n",
    "предсказать, если не известен её вход[6]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize(text, split= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords(text, ratio=0.05, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "# to create the bigrams\n",
    "bigram_model = Phrases(texts)\n",
    "import codecs\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open('bigram_sentences_all.txt', 'w', encoding='utf_8') as f:        \n",
    "        for unigram_sentence in texts:            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])            \n",
    "            f.write(bigram_sentence + '\\n')\n",
    "            \n",
    "from gensim.models.word2vec import LineSentence\n",
    "#bigram_sentences = LineSentence('bigram_sentences_all.txt')\n",
    "#bigram_sentences = open('bigram_sentences_all.txt', \"r\").read().splitlines()\n",
    "\n",
    "\n",
    "import codecs\n",
    "fileObj = codecs.open( 'bigram_sentences_all.txt', \"r\", \"utf_8_sig\" )\n",
    "bigram_sentences = fileObj.read().splitlines() # или читайте по строке\n",
    "fileObj.close()\n",
    "\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "triram_sentences = []\n",
    "i =0\n",
    "for bigram_sentence in bigram_sentences:\n",
    "    i += 1\n",
    "    triram_sentence = u' '.join(trigram_model[tokenize_only(bigram_sentence)])\n",
    "    triram_sentences.append(triram_sentence)\n",
    "    \n",
    "    #if len(triram_sentences) != i:\n",
    "        #triram_sentence = u' '.join(' ')\n",
    "print (i, len(triram_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
